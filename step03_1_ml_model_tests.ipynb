{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attached-coach",
   "metadata": {},
   "source": [
    "## Libraries and versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-watershed",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "affiliated-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_version = !Python -V #version 3.7.9\n",
    "import pandas as pd #version 1.2.3\n",
    "import numpy as np #version 1.19.2\n",
    "\n",
    "\n",
    "#for machine learning models\n",
    "#pre processing\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder, __version__ as ce_version #version 2.2.2\n",
    "\n",
    "#split data in train and test\n",
    "#version\n",
    "from sklearn import __version__ as sk_version #version 0.24.1\n",
    "\n",
    "#pre-processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#classificators\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB, ComplementNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#baseline\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-fiction",
   "metadata": {},
   "source": [
    "### Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "military-motor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.9\n",
      "Pandas version: 1.2.3\n",
      "Numpy version: 1.19.2\n",
      "Category Encoders version: 2.2.2\n",
      "Sklearn version: 0.24.1\n"
     ]
    }
   ],
   "source": [
    "print(f'{python_version[0]}')\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "print(f'Category Encoders version: {ce_version}')\n",
    "print(f'Sklearn version: {sk_version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-dinner",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "crucial-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-picking",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "norman-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('train_cleaning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mature-abraham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ever_Married</th>\n",
       "      <th>Age</th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Spending_Score</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Var_1</th>\n",
       "      <th>Segmentation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>462809</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>22</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Cat_4</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>462643</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Average</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Cat_4</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>466315</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>67</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Cat_6</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>461735</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>67</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>High</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Cat_6</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>462669</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>40</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>1.0</td>\n",
       "      <td>High</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Cat_6</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Gender Ever_Married  Age Graduated     Profession  Work_Experience  \\\n",
       "0  462809    Male           No   22        No     Healthcare              1.0   \n",
       "1  462643  Female          Yes   38       Yes       Engineer              1.0   \n",
       "2  466315  Female          Yes   67       Yes       Engineer              1.0   \n",
       "3  461735    Male          Yes   67       Yes         Lawyer              0.0   \n",
       "4  462669  Female          Yes   40       Yes  Entertainment              1.0   \n",
       "\n",
       "  Spending_Score  Family_Size  Var_1 Segmentation  \n",
       "0            Low          4.0  Cat_4            D  \n",
       "1        Average          3.0  Cat_4            A  \n",
       "2            Low          1.0  Cat_6            B  \n",
       "3           High          2.0  Cat_6            B  \n",
       "4           High          6.0  Cat_6            A  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-swaziland",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-outside",
   "metadata": {},
   "source": [
    "OPTIONS\n",
    "\n",
    "Treatment of categorical variables:\n",
    "\n",
    "variables = Profession, Spending_Score and Var_1\n",
    "- onehotencoder\n",
    "- ordinal\n",
    "\n",
    "Models\n",
    "- kneighborsclassifier\n",
    "- RadiusNeighborsClassifier\n",
    "- RandomForestClassifier\n",
    "- DecisionTreeClassifier\n",
    "- ExtraTreesClassifier\n",
    "- CategoricalNB\n",
    "- ComplementNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-preview",
   "metadata": {},
   "source": [
    "# Dataset transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-newsletter",
   "metadata": {},
   "source": [
    "## Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-judges",
   "metadata": {},
   "source": [
    "### OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "random-birmingham",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#OrdinalEncoder transform dataset using a map for each variable\n",
    "map_encoders = [\n",
    "{'col':'Ever_Married','mapping':{'No':0, 'Yes':1}},\n",
    "{'col':'Graduated','mapping':{'No':0, 'Yes':1}},\n",
    "{'col':'Profession','mapping':{'Homemaker':0, 'Doctor':4, 'Marketing':1, 'Healthcare':7, 'Entertainment':6,\n",
    "                               'Engineer':5, 'Artist':8, 'Lawyer':3, 'Executive':2}},\n",
    "{'col':'Spending_Score','mapping':{'Low':0, 'Average':1, 'High':2}},\n",
    "{'col':'Var_1','mapping':{'Cat_1':1, 'Cat_2':3, 'Cat_3':4, 'Cat_4':5, 'Cat_5':0, 'Cat_6':6, 'Cat_7':2}},\n",
    "{'col': 'Segmentation', 'mapping':{'A':0, 'B':1, 'C':2, 'D':3}}\n",
    "           ]\n",
    "encoder = OrdinalEncoder(cols=['Ever_Married', 'Graduated','Profession', 'Spending_Score', 'Var_1', 'Segmentation'],\n",
    "                        mapping=map_encoders)\n",
    "dataset_ordinal = encoder.fit_transform(dataset)\n",
    "\n",
    "#drop non used columns ID, Gender and Family_Size\n",
    "dataset_ordinal.drop(columns=['ID', 'Gender', 'Family_Size'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-vector",
   "metadata": {},
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "breeding-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OneHotEncoder\n",
    "encoder = OneHotEncoder(cols=['Ever_Married', 'Graduated','Profession', 'Spending_Score', 'Var_1'],\n",
    "                       use_cat_names=True)\n",
    "dataset_onehorencoder = encoder.fit_transform(dataset)\n",
    "\n",
    "#map column Segmentation and transform for numeric variables\n",
    "map_segmentation = {'A':0, 'B':1, 'C':2, 'D':3}\n",
    "dataset_onehorencoder['Segmentation'] = dataset_onehorencoder['Segmentation'].map(map_segmentation)\n",
    "\n",
    "#drop non used columns ID, Gender and Family_Size\n",
    "dataset_onehorencoder.drop(columns=['ID','Gender', 'Family_Size'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-bottom",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-cinema",
   "metadata": {},
   "source": [
    "### OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "happy-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ordinal = dataset_ordinal[['Ever_Married', 'Age', 'Graduated', 'Profession', 'Work_Experience', 'Spending_Score', 'Var_1']]\n",
    "y_ordinal = dataset_ordinal['Segmentation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-california",
   "metadata": {},
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "suitable-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selection of all variables, except Segmentation\n",
    "X_onehotencoder = dataset_onehorencoder.loc[:, dataset_onehorencoder.columns != 'Segmentation']\n",
    "y_onehotencoder = dataset_onehorencoder['Segmentation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-holmes",
   "metadata": {},
   "source": [
    "## Test 1 - OneHotEnconder and Kneighborsclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_onehotencoder, y_onehotencoder, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test1 = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "    test1.fit(X_train, y_train)\n",
    "    y_predict = test1.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-falls",
   "metadata": {},
   "source": [
    "## Test 2 - Ordinal and Kneighborsclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_ordinal, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test2 = KNeighborsClassifier(n_neighbors=4, weights='distance')\n",
    "    test2.fit(X_train, y_train)\n",
    "    y_predict = test2.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-subcommittee",
   "metadata": {},
   "source": [
    "## Test 3 - OneHotEncoder and RadiusNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_onehotencoder, y_onehotencoder, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test3 = RadiusNeighborsClassifier(n_neighbors=4, weights='distance', outlier_label='most_frequent')\n",
    "    test3.fit(X_train, y_train)\n",
    "    y_predict = test3.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-soviet",
   "metadata": {},
   "source": [
    "## Test 4 - OrdinalEncoder and RadiusNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-submission",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_ordinal, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test4 = RadiusNeighborsClassifier(n_neighbors=4, weights='distance', outlier_label='most_frequent')\n",
    "    test4.fit(X_train, y_train)\n",
    "    y_predict = test4.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-latin",
   "metadata": {},
   "source": [
    "## Test 5 - OneHotEncoder and RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-humanity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_onehotencoder, y_onehotencoder, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test5 = RandomForestClassifier(max_depth=4, min_samples_leaf=30)\n",
    "    test5.fit(X_train, y_train)\n",
    "    y_predict = test5.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-austria",
   "metadata": {},
   "source": [
    "## Test 6 - OrdinalEncoder and RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-pizza",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_ordinal, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test6 = RandomForestClassifier(max_depth=4, min_samples_leaf=30)\n",
    "    test6.fit(X_train, y_train)\n",
    "    y_predict = test6.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-virgin",
   "metadata": {},
   "source": [
    "## Test 7 - OneHotEncoder and DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-expression",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_onehotencoder, y_onehotencoder, train_size=0.9,\n",
    "                                                        stratify=y_onehotencoder)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test7 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=30)\n",
    "    test7.fit(X_train, y_train)\n",
    "    y_predict = test7.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-relation",
   "metadata": {},
   "source": [
    "## Test 8 - OrdinalEncoder and DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-little",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_ordinal, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test8 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=30)\n",
    "    test8.fit(X_train, y_train)\n",
    "    y_predict = test8.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-outside",
   "metadata": {},
   "source": [
    "## Test 9 - OneHotEncoder and ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-equivalent",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_onehotencoder, y_onehotencoder, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test7 = ExtraTreesClassifier(max_depth=4, min_samples_leaf=30)\n",
    "    test7.fit(X_train, y_train)\n",
    "    y_predict = test7.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-stand",
   "metadata": {},
   "source": [
    "## Test 10 - OrdinalEncoder and ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-mambo",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_ordinal, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test10 = ExtraTreesClassifier(max_depth=4, min_samples_leaf=30)\n",
    "    test10.fit(X_train, y_train)\n",
    "    y_predict = test10.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-dominican",
   "metadata": {},
   "source": [
    "## Test 11 - OneHotEncoder and CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_onehotencoder, y_onehotencoder, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test11 = CategoricalNB()\n",
    "    test11.fit(X_train, y_train)\n",
    "    y_predict = test11.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-stake",
   "metadata": {},
   "source": [
    "## Test 12 - OrdinalEncoder and CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_ordinal, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test12 = CategoricalNB()\n",
    "    test12.fit(X_train, y_train)\n",
    "    y_predict = test12.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-terminology",
   "metadata": {},
   "source": [
    "## Test 13 - OneHotEncoder and ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_onehotencoder, y_onehotencoder, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test13 = ComplementNB()\n",
    "    test13.fit(X_train, y_train)\n",
    "    y_predict = test13.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-charleston",
   "metadata": {},
   "source": [
    "## Test 14 - OrdinalEncoder and ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-representative",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_ordinal, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test14 = ComplementNB()\n",
    "    test14.fit(X_train, y_train)\n",
    "    y_predict = test14.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-mills",
   "metadata": {},
   "source": [
    "## Test 15 - OneHotEncoder and LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "unauthorized-period",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY FOR 100 ITERATIONS\n",
      "Max = 0.4841\n",
      "Mean = 0.4575\n",
      "Min = 0.4368\n",
      "Std = 0.0077\n"
     ]
    }
   ],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_onehotencoder, y_onehotencoder, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test15 = SVC()\n",
    "    test15.fit(X_train, y_train)\n",
    "    y_predict = test15.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-tablet",
   "metadata": {},
   "source": [
    "## Test 16 - OrdinalEncoder and LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interstate-norfolk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY FOR 100 ITERATIONS\n",
      "Max = 0.4759\n",
      "Mean = 0.4551\n",
      "Min = 0.4350\n",
      "Std = 0.0087\n"
     ]
    }
   ],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_ordinal, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test15 = SVC()\n",
    "    test15.fit(X_train, y_train)\n",
    "    y_predict = test15.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-basics",
   "metadata": {},
   "source": [
    "## Baseline (sklearn.dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-sample",
   "metadata": {},
   "source": [
    "### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_onehotencoder, y_onehotencoder, train_size=0.7)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test_dummy_1 = DummyClassifier(strategy='uniform')\n",
    "    test_dummy_1.fit(X_train, y_train)\n",
    "    y_predict = test_dummy_1.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-stylus",
   "metadata": {},
   "source": [
    "### OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first - split train and test data\n",
    "accuracy_list = []\n",
    "iterations = 100\n",
    "for iter in range(0, iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ordinal, y_ordinal, train_size=0.7)\n",
    "\n",
    "    #second - transform independet variables (train and test)\n",
    "    encoder = OrdinalEncoder(cols=['Ever_Married', 'Graduated','Profession', 'Spending_Score', 'Var_1'])\n",
    "    X_train = encoder.fit_transform(X_train)\n",
    "    X_test = encoder.fit_transform(X_test)\n",
    "\n",
    "    #third - macinhe learning apply\n",
    "    test_dummy_2 = DummyClassifier(strategy='uniform')\n",
    "    test_dummy_2.fit(X_train, y_train)\n",
    "    y_predict = test_dummy_2.predict(X_test)\n",
    "\n",
    "    #fourth - test accuray\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "max_accuracy = np.asarray(accuracy_list).max()\n",
    "mean_accuracy = np.asarray(accuracy_list).mean()\n",
    "min_accuracy = np.asarray(accuracy_list).min()\n",
    "std_accuracy = np.asarray(accuracy_list).std()\n",
    "print(f'ACCURACY FOR {iterations} ITERATIONS')\n",
    "print(f'Max = {max_accuracy:.4f}\\nMean = {mean_accuracy:.4f}\\nMin = {min_accuracy:.4f}\\nStd = {std_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-ozone",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-estimate",
   "metadata": {},
   "source": [
    "- test 6 reach the best scores (higher mean and minimum standard deviation) in 100 iterations\n",
    "- test 12 presented an average close to test 6, but with a smaller standard deviation\n",
    "- the scores of tests 6 and 12 is 100% bigger then a dummy classifier\n",
    "- seems that algorithm CategoricalNB is more fast them RandomForestClassifier with same accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-fault",
   "metadata": {},
   "source": [
    "## Next step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-biography",
   "metadata": {},
   "source": [
    "- work parameters to increase model accuracy for tests 6 and 12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
